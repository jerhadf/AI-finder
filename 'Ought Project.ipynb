{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"name":"ought_project.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"2883ce25738b44e2899d05586ca9bf9f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c802bb07a7304a7b812d7ad3fbe2a437","IPY_MODEL_86b15c274b104c1ab5e2c49d3b666be4","IPY_MODEL_e88e2b653148494390d7f9d6c8b981fb"],"layout":"IPY_MODEL_09d68d1608394d569c17d5e019c2ac5f"}},"c802bb07a7304a7b812d7ad3fbe2a437":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6cee53b817934dd59bda8a32b3402dc4","placeholder":"​","style":"IPY_MODEL_fea06615f074445e842758af727a8ecf","value":"Downloading:  95%"}},"86b15c274b104c1ab5e2c49d3b666be4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_80203640dd8443e8b971e57d87459454","max":44541580809,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1afeb392d0624522b20b93e3d631c44d","value":42312459264}},"e88e2b653148494390d7f9d6c8b981fb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_14115e42184e4bd7ae3658ea74cc958d","placeholder":"​","style":"IPY_MODEL_c91554ee41a04207b11ad72ba3d2527e","value":" 39.4G/41.5G [17:40&lt;00:40, 55.1MB/s]"}},"09d68d1608394d569c17d5e019c2ac5f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6cee53b817934dd59bda8a32b3402dc4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fea06615f074445e842758af727a8ecf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"80203640dd8443e8b971e57d87459454":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1afeb392d0624522b20b93e3d631c44d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"14115e42184e4bd7ae3658ea74cc958d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c91554ee41a04207b11ad72ba3d2527e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat_minor":0,"nbformat":4,"cells":[{"cell_type":"markdown","source":["Before running, choose Runtime > Change runtime type and select GPU.\n"],"metadata":{"id":"k7Zm7oSAsOZh"}},{"cell_type":"markdown","source":["# Task description"],"metadata":{"id":"ib8DExilGYtT"}},{"cell_type":"markdown","source":["The goal is to classify whether an ArXiv paper is AI-relevant or not. There’s a train, dev, and test set, each with 500 entries. The test set doesn’t have labels. We provide all these data points to help you evaluate your solution, but as described above the solution should work with only 20 labeled examples.'\n","\n","Test how well GPT-2 performs on it when applied in a straightforward way (few-shot learning with examples in prompt).\n","Experiment with changes that may improve it (e.g. adjustments to the prompt, using GPT-2 as part of more complex schemes, other models and training methods).\n","Don’t fine-tune GPT-2 or another model on the full training set, since in practice you will only have 20 labeled data points.\n","\n","Deliverables:\n","Share a writeup with you findings on:\n","* How well was GPT-2 able to perform on this task?\n","* What tweaks that you tried worked vs. didn’t work?\n","* What would you recommend based on these results? \n","* What would be good next steps?\n","Classify the test set and share a jsonl with the classifications.\n","Share your code.\n","\n","We’ll evaluate your project using these criteria, in approximate order of importance:\n","* Code quality (readability, extensibility)\n","* Optimization process for improving performance\n","* Communication regarding results and recommendations\n","* Performance on the classification task"],"metadata":{"id":"tnW8kxP-Gfyf"}},{"cell_type":"markdown","source":["# Setup\n","\n"],"metadata":{"id":"eSq1IwAw_slh"}},{"cell_type":"code","source":["!pip install transformers\n","!pip install torch\n","!pip install sentencepiece # to fix errors in T0pp section"],"metadata":{"id":"JPTo4DZW-8my","outputId":"99963ec2-9a18-4617-b091-d95dbc061ae1","execution":{"iopub.status.busy":"2022-05-29T17:44:58.821891Z","iopub.execute_input":"2022-05-29T17:44:58.822340Z","iopub.status.idle":"2022-05-29T17:45:18.601489Z","shell.execute_reply.started":"2022-05-29T17:44:58.822261Z","shell.execute_reply":"2022-05-29T17:45:18.600524Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1654128047029,"user_tz":240,"elapsed":8493,"user":{"displayName":"jeremy h","userId":"13438703414107206507"}}},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.19.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.7.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.4)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.5.18.1)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.11.0+cu113)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.2.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.96)\n"]}]},{"cell_type":"code","source":["from transformers import (set_seed,\n","                          GPT2Config,\n","                          GPT2Tokenizer,\n","                          GPT2LMHeadModel,\n","                          pipeline)\n","from sklearn.metrics import classification_report, accuracy_score\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","import json\n","import numpy as np\n","import pandas as pd\n","import random\n","from tqdm.notebook import tqdm\n","import collections\n","from nltk.tokenize import sent_tokenize, word_tokenize \n","from nltk.corpus import stopwords"],"metadata":{"id":"XHD836Xw_rLh","execution":{"iopub.status.busy":"2022-05-29T17:45:18.605864Z","iopub.execute_input":"2022-05-29T17:45:18.606158Z","iopub.status.idle":"2022-05-29T17:45:27.678984Z","shell.execute_reply.started":"2022-05-29T17:45:18.606129Z","shell.execute_reply":"2022-05-29T17:45:27.678212Z"},"trusted":true,"executionInfo":{"status":"ok","timestamp":1654128224461,"user_tz":240,"elapsed":6406,"user":{"displayName":"jeremy h","userId":"13438703414107206507"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["# Data processing"],"metadata":{"id":"OAvJT2yVdrNE"}},{"cell_type":"markdown","source":["Here I define a class for parsing the datasets and read in each data file."],"metadata":{"id":"tL04ouSbHrLx"}},{"cell_type":"code","source":["class Dataset():\n","    '''\n","    Class for parsing in the data and retrieving info about it\n","    '''\n","    def __init__(self, path):\n","        self.texts = []\n","        self.labels = []\n","\n","        def load_jsonl(filename):\n","            ''' Load JSONL file and read it with split lines '''\n","            f = open(filename)\n","            return [json.loads(line) for line in f.read().splitlines()]\n","\n","        self.data = load_jsonl(path) # load training data as list of dicts\n","\n","        # number of examples \n","        self.n_examples = len(self.data)\n","        return\n","\n","    def __len__(self):\n","        ''' Return number of examples '''\n","        return self.n_examples\n","\n","    def __getitem__(self, item):\n","        ''' \n","        Given an index return an example from the position.\n","        :param item (int): Index position to pick an example to return.\n","        :return Dict[str, str]: dictionary of inputs that contain text w/ labels\n","        '''\n","        return {'text':self.texts[item],\n","                'label':self.labels[item]}\n","\n","    def get_examples(self):\n","        '''\n","        Get positive (AI-relevant) and negative (non-AI) examples from training data\n","        '''\n","        true_AI = [x for x in self.data if x['label'] == 'True']\n","        false_AI = [x for x in self.data if x['label'] == 'False']\n","        return true_AI, false_AI\n","\n","    def get_rand_examples(self, amount_pos, amount_neg):\n","        '''\n","        Choose a random sample of positive & negative examples\n","        '''\n","        # get AI-relevant and AI-irrelevant texts from method of this class\n","        true_AI, false_AI = self.get_examples()\n","\n","        # get random sample of examples\n","        pos_examples = random.sample(true_AI, amount_pos)\n","        neg_examples = random.sample(false_AI, amount_neg)\n","\n","        # merge the examples together and return the result\n","        return pos_examples + neg_examples\n","\n","    def get_sample(self, n_samples=2):\n","        '''\n","        Get uniform randomly sampled examples from the data (with both true and false examples)\n","        '''\n","        if n_samples == 1:\n","            return random.choice(self.data)\n","        elif (n_samples % 2) != 0:\n","            div_odd = lambda n: (n//2, n//2 + 1)\n","            pos, neg = div_odd(n_samples)\n","        else:\n","            pos = n_samples // 2; neg = n_samples // 2\n","\n","        return self.get_rand_examples(pos, neg)"],"metadata":{"id":"U702tR7HMQiW","execution":{"iopub.status.busy":"2022-05-29T17:45:27.698423Z","iopub.execute_input":"2022-05-29T17:45:27.700710Z","iopub.status.idle":"2022-05-29T17:45:27.716448Z","shell.execute_reply.started":"2022-05-29T17:45:27.700341Z","shell.execute_reply":"2022-05-29T17:45:27.715548Z"},"trusted":true,"executionInfo":{"status":"ok","timestamp":1654128229442,"user_tz":240,"elapsed":128,"user":{"displayName":"jeremy h","userId":"13438703414107206507"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# mount google drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5xdXI0T3H5RF","executionInfo":{"status":"ok","timestamp":1654127879086,"user_tz":240,"elapsed":13280,"user":{"displayName":"jeremy h","userId":"13438703414107206507"}},"outputId":"95a17265-1fa8-4d63-b70f-5e05aa9a5011"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# save data folder path \n","data_path = \"/content/drive/MyDrive/Colab/OughtProject/data\"\n","\n","# save filepath variables\n","train_path = f\"{data_path}/train.jsonl\"\n","test_path = f\"{data_path}/test_no_labels.jsonl\"\n","dev_path = f\"{data_path}/dev.jsonl\"\n","\n","# create Dataset objects for each data file\n","train_data = Dataset(path=train_path)\n","test_data = Dataset(path=test_path)\n","dev_data = Dataset(path=dev_path)"],"metadata":{"id":"x-mM7PQDHxm4","executionInfo":{"status":"ok","timestamp":1654128233391,"user_tz":240,"elapsed":151,"user":{"displayName":"jeremy h","userId":"13438703414107206507"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["# Applying GPT-2 with Few-Shot Learning\n"],"metadata":{"id":"QXekQkwa-2oO"}},{"cell_type":"markdown","source":["## GPT-2 Language Model Initialization"],"metadata":{"id":"V7g5xuEV_zoU"}},{"cell_type":"markdown","source":["Resources:\n","\n","1.   https://huggingface.co/transformers/model_doc/gpt2.html#gpt2lmheadmodel\n","2.   https://github.com/huggingface/transformers/blob/master/examples/pytorch/text-generation/run_generation.py\n","\n","\n"],"metadata":{"id":"3AT8bTfeRDeL"}},{"cell_type":"markdown","source":["The following class refactors the original utility functions into a single GPT-2 language model class. "],"metadata":{"id":"wNSNsclhobJ1"}},{"cell_type":"code","source":["class GPT2LM:\n","    '''\n","    Superclass for GPT-2 language models. \n","    '''\n","    def __init__(self):\n","        # set seed for simple reproduction of these results\n","        set_seed(42)\n","\n","        # truncate text sequences to specific (tokenized) length (set to gpt2-xl)\n","        self.max_length = 1024 # if None use max length allowed by model\n","\n","        # set model and default device\n","        self.model_name = 'gpt2'\n","        \n","        # create GPT language model from HuggingFace pretrained models\n","        self.tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","        self.tokenizer.pad_token = self.tokenizer.eos_token\n","        self.model = GPT2LMHeadModel.from_pretrained('gpt2',pad_token_id=self.tokenizer.eos_token_id)\n","        self.model.eval().cuda()\n","\n","    def generate(self, prompt, max_length=5, stop_token=None):\n","        '''\n","        Generate a list of words to autocomplete a prompt. \n","        :param prompt: The prompt to complete.\n","        :param max_length: The maximum number of words to generate.\n","        :param stop_token: A token to stop generating words.\n","        :return: A list of words to complete the prompt.\n","        '''\n","        # tokenize the prompt text and save input_ids [0], token_type_id [1], and attention_mask [2]\n","        input_ids = self.tokenizer.encode(prompt,\n","                                          truncation=True,\n","                                          return_tensors=\"pt\")\n","        \n","        # generate input text IDs for the model\n","        generated_text_ids = self.model.generate(input_ids=input_ids.cuda(), \n","                                            max_length=self.max_length, \n","                                            do_sample=False)\n","        \n","        # decode the generated text from the tokenizer\n","        generated_text = self.tokenizer.decode(generated_text_ids[0], \n","                                          clean_up_tokenization_spaces=True)\n","        post_prompt_text = generated_text[len(self.tokenizer.decode(input_ids[0], \n","                                                                    clean_up_tokenization_spaces=True)):]\n","        # return the prompt plus the generated text\n","        return prompt + post_prompt_text[:post_prompt_text.find(stop_token) if stop_token else None]\n","    \n","\n","    def get_logits_and_tokens(self, text):\n","        '''\n","        Returns the logits and tokens for the given text\n","        '''\n","        input_ids = self.tokenizer.encode(text, return_tensors=\"pt\")\n","        tokens = [self.tokenizer.decode([input_id]) for input_id in input_ids[0]]\n","        output = self.model(input_ids.cuda())\n","        return output.logits[0][:-1], tokens"],"metadata":{"execution":{"iopub.status.busy":"2022-05-29T17:47:43.185377Z","iopub.execute_input":"2022-05-29T17:47:43.185899Z","iopub.status.idle":"2022-05-29T17:47:43.214976Z","shell.execute_reply.started":"2022-05-29T17:47:43.185865Z","shell.execute_reply":"2022-05-29T17:47:43.214067Z"},"trusted":true,"id":"9xP053RTGYtZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Define GPT-2 Language Model Classifier"],"metadata":{"id":"ZJp_NAKbHe1Z"}},{"cell_type":"code","source":["class GPTClassifier(GPT2LM):\n","    '''\n","    Classify papers as AI-relevant or not with GPT-2 few-shot learning\n","    '''\n","    def __init__(self, dataset, num_examples=3, default_examples=None):\n","        self.instructions = '''\"Classify each example paper as 'AI' or 'Not AI':\\n--'''\n","\n","        # inherit the GPT2LM superclass \n","        super(GPTClassifier, self).__init__()\n","\n","        # save data and dataset as class variables\n","        self.dataset = dataset\n","        self.data = dataset.data\n","\n","        if default_examples is None:\n","            # get a number of examples from the Dataset object passed to this method\n","            self.examples = dataset.get_sample(num_examples)\n","        else:\n","            # set the examples to the default examples (used for test data)\n","            self.examples = default_examples\n","            \n","    def render_example(self, example):\n","        ''' Render an example of a paper '''\n","        # take the title & abstract texts and strip them \n","        title = example[\"text\"].split(\".\")[0].strip()\n","        abstract = example[\"text\"][len(title)+1:].strip()\n","        # return the title, abstract, and label of paper (if AI-relevant or not)\n","        label = \"AI\" if example[\"label\"] == \"True\" else \"Not AI\"\n","        rendered = f\"\"\"Title: {title}\\nAbstract: {abstract}\\nLabel: {label}\"\"\"\n","        return rendered\n","\n","    def render_end_example(self, example):\n","        ''' Render end example, with predicted label '''\n","        title = example[\"text\"].split(\".\")[0].strip()\n","        abstract = example[\"text\"][len(title)+1:].strip()\n","        rendered = f\"\"\"Title: {title}\\nAbstract: {abstract}\\nLabel:\"\"\"\n","        return rendered\n","    \n","    def make_prompt(self, instructions, examples, end_example):\n","        ''' Print a series of examples to make a prompt for GPT-2 '''\n","        rendered_examples = \"\\n\\n--\\n\\n\".join([self.render_example(example) for example in examples])\n","        return f\"\"\"{instructions}\n","        {rendered_examples}\n","        --\n","        {self.render_end_example(end_example)}\"\"\"\n","    \n","    def classify(self, text):\n","        ''' Classify a given text as AI relevant or not '''\n","        prompt = self.make_prompt(self.instructions, self.examples, text)\n","        gen_text = self.generate(prompt, stop_token=\"\\n\")\n","\n","        # get the prediction from the generated text\n","        pred = gen_text.split('\\n')[-1].strip(\"Label: \")\n","        return pred\n","    \n","    def evaluate(self, samples=50):\n","        ''' Evaluate the model's accuracy based on a given number of samples '''\n","        samples = self.dataset.get_sample(samples)\n","\n","        hits = [] # list to keep track of prediction accuracy\n","        for sample in samples:\n","            response = self.classify(sample)\n","            # create dictionary to convert predictions to True/False\n","            response_dict = {\n","                'AI': 'True',\n","                'Not AI': 'False'\n","            }\n","            pred = response_dict.get(response, 'Invalid response')\n","            real = sample['label']\n","            hits.append(pred == real)\n","    \n","        # calculate accuracy and return\n","        accuracy = np.array(hits).sum() / len(hits)\n","        print(f\"Model accuracy: {accuracy * 100}%\")\n","        return accuracy\n","\n","    def classify_dataset(self, num = None):\n","        ''' \n","        Classify all the papers in the dataset and save as a dictionary \n","        :param num (int): number of papers to classify (default to length of dataset when None)\n","        '''\n","        # if no number specified, classify entire dataset; else, classify up to the number\n","        paper_list = self.data if num is None else self.data[:num] \n","            \n","        results = [] # create empty list to store all papers (with predictions)\n","        for paper in paper_list:\n","            pred = self.classify(paper)\n","            print(f\"Classification result for paper '{paper['text'].split(' ')[0]}': {pred}\")\n","            paper[\"label\"] = pred # add prediction to paper \n","            results.append(paper)\n","            \n","        return results "],"metadata":{"id":"c7oAanTUrxxN","execution":{"iopub.status.busy":"2022-05-29T17:51:13.395241Z","iopub.execute_input":"2022-05-29T17:51:13.395720Z","iopub.status.idle":"2022-05-29T17:51:13.425317Z","shell.execute_reply.started":"2022-05-29T17:51:13.395687Z","shell.execute_reply":"2022-05-29T17:51:13.424352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Initialize and evaluate classifier "],"metadata":{"id":"NxdWH5nuJAvD"}},{"cell_type":"code","source":["# initialize the classifier with the training data\n","classifier = GPTClassifier(train_data, num_examples=3)"],"metadata":{"execution":{"iopub.status.busy":"2022-05-29T17:47:59.103087Z","iopub.execute_input":"2022-05-29T17:47:59.103592Z","iopub.status.idle":"2022-05-29T17:48:09.372057Z","shell.execute_reply.started":"2022-05-29T17:47:59.103557Z","shell.execute_reply":"2022-05-29T17:48:09.371235Z"},"trusted":true,"id":"txBdOHx7GYtc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# example classification - choose a paper to classify and predict it \n","text = train_data.data[5] # should be False ('Not AI') for 5\n","pred = classifier.classify(text)\n","print(pred)"],"metadata":{"id":"FJgOFYbNwIqH","outputId":"899a6cac-c3d1-4f61-e7c7-5ba654779c49","execution":{"iopub.status.busy":"2022-05-29T17:48:12.805018Z","iopub.execute_input":"2022-05-29T17:48:12.805648Z","iopub.status.idle":"2022-05-29T17:48:13.987036Z","shell.execute_reply.started":"2022-05-29T17:48:12.805602Z","shell.execute_reply":"2022-05-29T17:48:13.986201Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1653869069044,"user_tz":240,"elapsed":824,"user":{"displayName":"jeremy h","userId":"13438703414107206507"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Not AI\n"]}]},{"cell_type":"code","source":["# evaluation - used 50 samples for actual evaluation, shortened to 5 to reduce runtime)\n","evaluated = classifier.evaluate(samples=5)\n","evaluated"],"metadata":{"id":"Xr9cBULe2RQ1","outputId":"8ade931a-2f14-4892-87d7-d72a1bc21ce5","execution":{"iopub.status.busy":"2022-05-29T17:48:16.883952Z","iopub.execute_input":"2022-05-29T17:48:16.884596Z","iopub.status.idle":"2022-05-29T17:49:06.106453Z","shell.execute_reply.started":"2022-05-29T17:48:16.884558Z","shell.execute_reply":"2022-05-29T17:49:06.105541Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1653869074003,"user_tz":240,"elapsed":4965,"user":{"displayName":"jeremy h","userId":"13438703414107206507"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Input length of input_ids is 1024, but ``max_length`` is set to 1024. This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n"]},{"output_type":"stream","name":"stdout","text":["Model accuracy: 60.0%\n"]},{"output_type":"execute_result","data":{"text/plain":["0.6"]},"metadata":{},"execution_count":10}]},{"cell_type":"markdown","source":["Depending on which samples are selected, this evaluation returns an accuracy between about 40% and 50%. "],"metadata":{"id":"2v7vwcRclxxH"}},{"cell_type":"code","source":["# initialize a classifier for predicting the test (no labels) dataset \n","# use examples from the training dataset for the GPT-2 prompt (2 AI-relevant, 2 AI-irrelevant examples)\n","pos, neg = train_data.get_examples()\n","train_examples = pos[:1] + neg[:1]\n","classifier = GPTClassifier(test_data, num_examples=3, default_examples=train_examples)"],"metadata":{"id":"qsHO8xxRPlqW","execution":{"iopub.status.busy":"2022-05-29T17:49:55.073043Z","iopub.execute_input":"2022-05-29T17:49:55.073943Z","iopub.status.idle":"2022-05-29T17:50:05.182599Z","shell.execute_reply.started":"2022-05-29T17:49:55.073896Z","shell.execute_reply":"2022-05-29T17:50:05.181752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# classify part of the test predictions dataset with GPT-2 few-shot learning\n","# (takes too long to classify the entire 500-line dataset with GPT-2, couldn't finish run in kaggle/colab)\n","test_predictions = classifier.classify_dataset(num=5)"],"metadata":{"execution":{"iopub.status.busy":"2022-05-29T17:50:47.709026Z","iopub.execute_input":"2022-05-29T17:50:47.709843Z","iopub.status.idle":"2022-05-29T17:51:04.515966Z","shell.execute_reply.started":"2022-05-29T17:50:47.709807Z","shell.execute_reply":"2022-05-29T17:51:04.514994Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"1WLMxkhqGYtd","executionInfo":{"status":"ok","timestamp":1653869096988,"user_tz":240,"elapsed":18329,"user":{"displayName":"jeremy h","userId":"13438703414107206507"}},"outputId":"44bb716a-9514-47f5-a449-2c58e765e9dd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Classification result for paper 'out': Not AI\n","Classification result for paper 'level': Not AI\n","Classification result for paper 'dbscan': Not AI\n","Classification result for paper 'spherical': Not AI\n","Classification result for paper 'rdec': Not AI\n"]}]},{"cell_type":"code","source":["def export_jsonl(predictions, filename='predictions.jsonl'):\n","    '''Function to convert a classified predictions dictionary to a jsonl file '''\n","    # convert to jsonl and save\n","    with open(filename, 'w') as f:\n","        for item in predictions:\n","            f.write(json.dumps(item))\n","            f.write('\\n')\n","    print(f\"Saved predictions to file {filename}\")\n","    return filename"],"metadata":{"execution":{"iopub.status.busy":"2022-05-29T17:51:25.932798Z","iopub.execute_input":"2022-05-29T17:51:25.933147Z","iopub.status.idle":"2022-05-29T17:51:25.938079Z","shell.execute_reply.started":"2022-05-29T17:51:25.933116Z","shell.execute_reply":"2022-05-29T17:51:25.937248Z"},"trusted":true,"id":"gr12j4gyGYte"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Keyword-Based Classification"],"metadata":{"id":"2p_McvNpGYte"}},{"cell_type":"markdown","source":["This simple approach uses domain-specific knowledge to classify papers as AI-relevant based on the presence of keywords. This simulates how a human might classify these papers - by scanning for keywords they recognize in the paper titles and abstracts. \n","\n","This approach can also serve as a baseline, as it is much more lightweight than a full-scale language mode and will run faster. If other models cannot outperform this baseline, it indicates that a simple dictionary of keywords can \n","Along with the manually added keywords, I scan all the papers in the dataset that are AI-relevant, and add some of the most common keywords to the keyword list.\n","\n","However, classification with keywords runs into the problem of generalizability - these keywords may be over-represented in this dataset compared to the full corpus of all AI-relevant papers. Further, there may be papers not relevant to AI that use these keywords, resulting in false positives. (Although arguably any paper that mentions ML or AI methods could be considered \"AI-relevant\"). "],"metadata":{"id":"kC1BDi__zLU7"}},{"cell_type":"code","source":["class KeywordClassifier:\n","    def __init__(self, dataset, auto_keywords=False):\n","        '''\n","        Classify papers as AI-relevant or not with domain-specific keyword list\n","        :param auto_keywords - if True, will use automated keyword selection using add_keywords()\n","        '''\n","        self.auto_keywords = auto_keywords\n","        self.dataset = dataset\n","        self.data = dataset.data\n","        # this list is manually compiled based on expected words & by looking at top words from add_keywords()\n","        self.keyword_list = [\"machine learning\", \"artificial intelligence\", \"gradient descent\",\n","                             \"neural network\", \"deep learning\", \"transformer\", \"natural language processing\",\n","                             \"classification\", \"convolutional\", \"thresholding\", \"corpus\", \"convolutional\",\n","                             \"embedding\", \"pooling\", \"reinforcement learning\"]\n","        \n","    def classify(self, text):\n","        ''' \n","        Classify a given text as AI relevant or not with the keyword list\n","        '''\n","        if self.auto_keywords:\n","            self.keyword_list = [word[0] for word in self.add_keywords(num_keywords = 20)]\n","        # check if the text contains any of the keywords\n","        text_contains_keyword = any(keyword in text for keyword in self.keyword_list)\n","        \n","        # return 'AI' if text contains keyword, return 'Not AI' if not\n","        return 'AI' if text_contains_keyword else 'Not AI'\n","    \n","    def add_keywords(self, num_keywords=10): \n","        '''\n","        Add keywords to the keyword list based on their frequency in the AI-relevant papers\n","        Warning: does not work with unlabeled data, will throw KeyError\n","        '''\n","        pos, neg = self.dataset.get_examples()\n","        keyword_list = [] # initialize list for storing all keywords from the text\n","        \n","        # add all words in each paper to the keyword list \n","        for paper in pos:\n","            # put all words in the text into a list (removing periods)\n","            word_list = paper[\"text\"].replace(\".\", \"\").split(\" \")\n","            keyword_list.extend(word_list)\n","        \n","        # remove stopwords from the list using ntlk \n","        stop_words = set(stopwords.words('english'))\n","        keyword_list = [word for word in keyword_list if word not in stop_words]\n","            \n","        # create Counter of the frequency of each keyword\n","        frequency = collections.Counter(keyword_list)\n","        self.freq_dict = dict(frequency) # convert the Counter to a dictionary and save it as class variable\n","        \n","        # get the most common N items in the frequency dict\n","        most_common = frequency.most_common(num_keywords)\n","        \n","        return most_common\n","        \n","    def evaluate(self, samples=None):\n","        ''' \n","        Evaluate the model's accuracy based on a given number of samples (or the full dataset)\n","        :param samples: if None, evaluate based on full dataset; if number, evaluate w/ that # of samples \n","        '''\n","        if samples is None: \n","            samples = self.dataset.data\n","        else: \n","            samples = self.dataset.get_sample(samples)\n","\n","        hits = [] # list to keep track of prediction accuracy\n","        for sample in samples:\n","            response = self.classify(sample['text'])\n","            # create dictionary to convert predictions to True/False\n","            response_dict = {\n","                'AI': 'True',\n","                'Not AI': 'False'\n","            }\n","            pred = response_dict.get(response, 'Invalid response')\n","            real = sample['label']\n","            hits.append(pred == real)\n","    \n","        # calculate accuracy and return\n","        accuracy = np.array(hits).sum() / len(hits)\n","        print(f\"Model accuracy: {accuracy * 100}%\")\n","        return accuracy\n","\n","    def classify_dataset(self, num = None):\n","        '''\n","        Classify all the papers in the dataset and save as a dictionary \n","        :param num (int): number of papers to classify (default to length of dataset when None)\n","        '''\n","        # if no number specified, classify entire dataset; else, classify up to the number\n","        paper_list = self.data if num is None else self.data[:num]\n","            \n","        results = [] # create empty list to store all papers (with predictions)\n","        for paper in paper_list:\n","            pred = self.classify(paper['text'])\n","            paper[\"label\"] = \"True\" if pred == \"AI\" else \"False\" # add prediction to paper \n","            results.append(paper)\n","            \n","        return results"],"metadata":{"execution":{"iopub.status.busy":"2022-05-29T17:51:33.542855Z","iopub.execute_input":"2022-05-29T17:51:33.543432Z","iopub.status.idle":"2022-05-29T17:51:33.561032Z","shell.execute_reply.started":"2022-05-29T17:51:33.543396Z","shell.execute_reply":"2022-05-29T17:51:33.560299Z"},"trusted":true,"id":"Kj8HMIUyGYte"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["keyword_classifier = KeywordClassifier(train_data)\n","keyword_classifier.evaluate()"],"metadata":{"execution":{"iopub.status.busy":"2022-05-29T17:51:35.450205Z","iopub.execute_input":"2022-05-29T17:51:35.451099Z","iopub.status.idle":"2022-05-29T17:51:35.471934Z","shell.execute_reply.started":"2022-05-29T17:51:35.451054Z","shell.execute_reply":"2022-05-29T17:51:35.471133Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"XqfgekvRGYte","executionInfo":{"status":"ok","timestamp":1653869096989,"user_tz":240,"elapsed":30,"user":{"displayName":"jeremy h","userId":"13438703414107206507"}},"outputId":"7e1f9426-8eb6-48b2-9418-49c8714b1df6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model accuracy: 91.60000000000001%\n"]},{"output_type":"execute_result","data":{"text/plain":["0.916"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["dev_keyword_classifier = KeywordClassifier(dev_data)\n","dev_keyword_classifier.evaluate()"],"metadata":{"execution":{"iopub.status.busy":"2022-05-29T17:51:37.603766Z","iopub.execute_input":"2022-05-29T17:51:37.604107Z","iopub.status.idle":"2022-05-29T17:51:37.619460Z","shell.execute_reply.started":"2022-05-29T17:51:37.604079Z","shell.execute_reply":"2022-05-29T17:51:37.618538Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"OPdfNdZ_GYtf","executionInfo":{"status":"ok","timestamp":1653869096989,"user_tz":240,"elapsed":23,"user":{"displayName":"jeremy h","userId":"13438703414107206507"}},"outputId":"48b32df0-374e-4f91-959d-e7ffb647ce82"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model accuracy: 92.0%\n"]},{"output_type":"execute_result","data":{"text/plain":["0.92"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["dev_keyword_classifier = KeywordClassifier(test_data)\n","keyword_predictions = dev_keyword_classifier.classify_dataset()\n","# export_jsonl(keyword_predictions, filename=\"keyword_predictions.jsonl\")"],"metadata":{"execution":{"iopub.status.busy":"2022-05-29T17:51:40.867933Z","iopub.execute_input":"2022-05-29T17:51:40.868675Z","iopub.status.idle":"2022-05-29T17:51:40.891616Z","shell.execute_reply.started":"2022-05-29T17:51:40.868641Z","shell.execute_reply":"2022-05-29T17:51:40.890699Z"},"trusted":true,"id":"4XgcFVwsGYtf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This demonstrates the effectiveness of simple keyword-based approach to text classification that leverages domain-specific knowledge about the field of artifical intelligence. The keyword classifier achieves a 90-92% accuracy on both the training and the dev data (these datasets combined contain 1000 total scientific papers). For comparison, humans only achieve about a 73% accuracy on the few-shot text classification tasks in the RAFT dataset ([source](https://paperswithcode.com/sota/few-shot-text-classification-on-raft)). While generalizability might be a concern for this approach, it seems reasonable to assume that most papers that contain these technical and domain-specific keywords are in fact relevant to artificial intelligence.  "],"metadata":{"id":"5D-1GQ6XGYtf"}},{"cell_type":"markdown","source":["# Zero-Shot Topic Classification with BART"],"metadata":{"id":"skwwjxuhJ7ZQ"}},{"cell_type":"markdown","source":["Finally, I try a few zero-shot topic classification approaches. These models do not require any examples at all, and achieve a surprisingly high classifiction accuracy for this dataset."],"metadata":{"id":"-L7uGc_Vm0XT"}},{"cell_type":"markdown","source":["Helpful resources used for this section: \n","\n","\n","* [Joe Davidson - Zero-Shot Learning in Modern NLP](https://joeddav.github.io/blog/2020/05/29/ZSL.html)\n","* [HuggingFace Implementation of Bart-Large-MNLI](https://huggingface.co/facebook/bart-large-mnli)\n","\n","\n","Resources for possible future work: \n","* [BERT with HuggingFace Transformers](https://www.kaggle.com/code/tuckerarrants/bert-with-huggingface-transformers#III.-BERT)\n","* [Topic Modeling in Python with BERTopic](https://hackernoon.com/nlp-tutorial-topic-modeling-in-python-with-bertopic-372w35l9) \n","* [Tutorial - Topic Modeling with BERTopic](https://colab.research.google.com/drive/1FieRA9fLdkQEGDIMYl0I3MCjSUKVF8C-?usp=sharing#scrollTo=QI6vwelqnTL-)\n","* [cuBERT-topic-modelling](https://github.com/rapidsai/rapids-examples/tree/main/cuBERT_topic_modelling)\n","* [Summarizing topic models with Transformers](https://www.kaggle.com/code/donkeys/summarizing-topic-models-with-transformers/notebook)\n","* [Low-resource NLP Bootcamp](https://github.com/neubig/lowresource-nlp-bootcamp-2020)"],"metadata":{"id":"57yRZKSinzC-"}},{"cell_type":"code","source":["# install bertopic for topic modeling with BERT\n","# !pip install bertopic"],"metadata":{"id":"Hw_911cWrZVB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# imports for this section\n","from transformers import BartForSequenceClassification, BartTokenizer\n","import requests\n","from bs4 import BeautifulSoup"],"metadata":{"id":"8PmzdpuNo_c-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class ZeroShotClassifier:\n","    def __init__(self, dataset, classify_method):\n","        '''\n","        Classify papers as AI-relevant or not using zero-shot learning. \n","        :param classify_type: The type of classification to perform\n","        '''\n","        self.dataset = dataset\n","        self.data = dataset.data\n","        self.classify_method = classify_method\n","        \n","    def classify(self, text):\n","        '''\n","        Classify a given text as AI relevant or not, using specified method\n","        '''\n","        if self.classify_method == \"hypothesis\": \n","            print(\"Classifying text with BART hypothesis prediction method\")\n","            return self.classify_hypothesis(text)\n","        else:\n","            print(\"Classifying text with BART topic classification method\")\n","            return self.classify_topic(text)\n","\n","    def classify_hypothesis(self, text, threshold=60):\n","        '''\n","        Classify the text using sequence-hypothesis prediction with BART\n","        This treats the paper as a premise, and the hypothesis as an assertion that \n","        the paper is about artificial intelligence. \n","        :arg threshold: if probability is above threshold, paper is AI-relevant\n","        :return: True if paper is AI-relevant, False if not \n","        '''\n","        # load model pretrained on MNLI\n","        tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-mnli')\n","        model = BartForSequenceClassification.from_pretrained('facebook/bart-large-mnli')\n","\n","        # pose sequence as a NLI premise and label (politics) as a hypothesis\n","        premise = f\"{text}?\"\n","        hypothesis = 'This text is relevant to artificial intelligence or machine learning.'\n","\n","        # encode the premise & hypothesis, then run model and retrieve logits\n","        input_ids = tokenizer.encode(premise, hypothesis, return_tensors='pt')\n","        logits = model(input_ids)[0]\n","\n","        # probability of \"entailment\" (2) is the probability of label being true \n","        entail_contradiction_logits = logits[:,[0,2]]\n","        probs = entail_contradiction_logits.softmax(dim=1)\n","        true_prob = probs[:,1].item() * 100\n","\n","        print(f\"\"\"Probability that the label is true for paper: {true_prob:0.2f}%\"\"\")\n","        \n","        return 'AI' if true_prob >= threshold else 'Not AI'\n","\n","    def classify_topic(self, text):\n","        '''\n","        Classify the text with BART-Large-MNLI zero-shot topic classification \n","        '''\n","        # initialize the BART model and set the labels to AI or Not AI\n","        classifier = pipeline(\"zero-shot-classification\", model = \"facebook/bart-large-mnli\")\n","        \n","        # get topics from the arxiv taxonomy\n","        topic_list = self.scrape_topics()\n","\n","        # classify the text with multi-label false (only one label is correct)\n","        result = classifier(text, topic_list, multi_label=False)\n","\n","        # extract scores and labels and add them to dict \n","        labels = result['labels']\n","        scores = result['scores'] \n","        res_dict = {label : score for label, score in zip(labels, scores)}\n","\n","        # get the label and value of the highest-scored item in the dict\n","        max_value = max(res_dict.values())\n","        max_label = max(res_dict, key=res_dict.get)\n","        print(f\"Highest-scored label = {max_label}:{max_value}\")\n","\n","        return 'AI' if max_label == 'Artificial Intelligence' else 'Not AI'\n","\n","    def scrape_topics(self, url=\"https://arxiv.org/category_taxonomy\"): \n","        '''\n","        Scrape list of paper topics from url\n","        Made for arxiv category_taxonomy page, will likely break with other URLs\n","        '''\n","        # get the html page and parse it with BeautifulSoup\n","        response = requests.get(url)\n","        soup = BeautifulSoup(response.text, 'html.parser')\n","\n","        span_list = soup.findAll(\"span\")[1:] # index past the title span\n","\n","        # get text and remove parantheses\n","        span_list_cln = [span.text.strip('()') for span in span_list]\n","\n","        # some spans are not topics, remove them - they all start lowercase\n","        topic_list = [word for word in span_list_cln if not word.islower()]\n","\n","        # this results in a list of 155 topics, too large for classification in colab \n","        # the full topic_list could be useful to improve performance with more compute\n","        # based on this list, manually truncate to just 10 overarching topics\n","        topic_list = [\"Artificial Intelligence\", \"Computer Science\", \"Economics\",\n","                      \"Electrical Engineering\", \"Mathematics\", \"Physics\", \n","                      \"Biology\", \"Neuroscience\", \"Finance\", \"Statistics\"]\n","\n","        return topic_list\n","          \n","    def evaluate(self, samples=10):\n","        ''' \n","        Evaluate the model's accuracy based on a given number of samples (or the full dataset)\n","        :param samples: if None, evaluate based on full dataset; if number, evaluate w/ that # of samples \n","        '''\n","        samples = self.dataset.get_sample(samples)\n","\n","        hits = [] # list to keep track of prediction accuracy\n","        for sample in samples:\n","            response = self.classify(sample['text'])\n","            # create dictionary to convert predictions to True/False\n","            response_dict = {\n","                'AI': 'True',\n","                'Not AI': 'False'\n","            }\n","            pred = response_dict.get(response, 'Invalid response')\n","            real = sample['label']\n","            hits.append(pred == real)\n","    \n","        # calculate accuracy and return\n","        accuracy = np.array(hits).sum() / len(hits)\n","        print(f\"Model accuracy: {accuracy * 100}%\")\n","        return accuracy\n","\n","    def classify_dataset(self, num = None):\n","        '''\n","        Classify all the papers in the dataset and save as a dictionary \n","        :param num (int): number of papers to classify (default to length of dataset when None)\n","        '''\n","        # if no number specified, classify entire dataset; else, classify up to the number\n","        paper_list = self.data if num is None else self.data[:num] \n","            \n","        results = [] # create empty list to store all papers (with predictions)\n","        for paper in paper_list:\n","            pred = self.classify(paper)\n","            print(f\"Classification result for paper '{paper['text'].split(' ')[0]}': {pred}\")\n","            paper[\"label\"] = pred # add prediction to paper \n","            results.append(paper)\n","        \n","        return results"],"metadata":{"id":"rlijEn2yGYtf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["hypothesis_classifier = ZeroShotClassifier(train_data, classify_method=\"hypothesis\")"],"metadata":{"execution":{"iopub.status.busy":"2022-05-29T18:25:24.937935Z","iopub.execute_input":"2022-05-29T18:25:24.938314Z","iopub.status.idle":"2022-05-29T18:27:39.492206Z","shell.execute_reply.started":"2022-05-29T18:25:24.938281Z","shell.execute_reply":"2022-05-29T18:27:39.490660Z"},"trusted":true,"id":"iOAMzdwQGYtg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The line below takes a long time to run, and has been shortened to just 5 samples. However, the ZeroShotClassifier model using the hypothesis method achieved a classification accuracy of 90.0% on a sample of 50 papers."],"metadata":{"id":"plX7mtoM-H5W"}},{"cell_type":"code","source":["evaluated = hypothesis_classifier.evaluate(samples=5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JQ7EEsHJwSOr","executionInfo":{"status":"ok","timestamp":1653869149207,"user_tz":240,"elapsed":51973,"user":{"displayName":"jeremy h","userId":"13438703414107206507"}},"outputId":"339d850f-e525-486e-d7f1-07e38e1f677b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Classifying text with BART hypothesis prediction method\n","Probability that the label is true for paper: 99.95%\n","Classifying text with BART hypothesis prediction method\n","Probability that the label is true for paper: 99.55%\n","Classifying text with BART hypothesis prediction method\n","Probability that the label is true for paper: 94.08%\n","Classifying text with BART hypothesis prediction method\n","Probability that the label is true for paper: 99.78%\n","Classifying text with BART hypothesis prediction method\n","Probability that the label is true for paper: 16.89%\n","Model accuracy: 60.0%\n"]}]},{"cell_type":"code","source":["# topic classification with BART-Large-MLNI, using the arxiv topic categories\n","topic_classifier = ZeroShotClassifier(train_data, classify_method=\"topic\")"],"metadata":{"id":"mcswJh0VsGmX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# classify a single example using the topic classification method\n","example = train_data.get_sample(1)['text']\n","\n","classified = topic_classifier.classify(example)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4-DiJD8QFhrI","executionInfo":{"status":"ok","timestamp":1653869176621,"user_tz":240,"elapsed":27436,"user":{"displayName":"jeremy h","userId":"13438703414107206507"}},"outputId":"65832a12-0a71-460c-c91b-66c85f67763d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Classifying text with BART topic classification method\n","Highest-scored label = Mathematics:0.2151394635438919\n"]}]},{"cell_type":"code","source":["evaluated = topic_classifier.evaluate(samples = 1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qVqg21mYO7Gl","executionInfo":{"status":"ok","timestamp":1653870565447,"user_tz":240,"elapsed":1388851,"user":{"displayName":"jeremy h","userId":"13438703414107206507"}},"outputId":"40c8ce7f-9b21-4abf-98dc-3058e12d293c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Classifying text with BART topic classification method\n","Highest-scored label = Artificial Intelligence:0.21841001510620117\n","Classifying text with BART topic classification method\n","Highest-scored label = Computer Science:0.21647033095359802\n","Classifying text with BART topic classification method\n","Highest-scored label = Computer Science:0.2620323896408081\n","Classifying text with BART topic classification method\n","Highest-scored label = Mathematics:0.3561461269855499\n","Classifying text with BART topic classification method\n","Highest-scored label = Statistics:0.18978750705718994\n","Classifying text with BART topic classification method\n","Highest-scored label = Neuroscience:0.25566476583480835\n","Classifying text with BART topic classification method\n","Highest-scored label = Artificial Intelligence:0.25085097551345825\n","Classifying text with BART topic classification method\n","Highest-scored label = Artificial Intelligence:0.21848642826080322\n","Classifying text with BART topic classification method\n","Highest-scored label = Computer Science:0.2514156401157379\n","Classifying text with BART topic classification method\n","Highest-scored label = Statistics:0.16926053166389465\n","Classifying text with BART topic classification method\n","Highest-scored label = Artificial Intelligence:0.3238813579082489\n","Classifying text with BART topic classification method\n","Highest-scored label = Computer Science:0.19985182583332062\n","Classifying text with BART topic classification method\n","Highest-scored label = Mathematics:0.5255776047706604\n","Classifying text with BART topic classification method\n","Highest-scored label = Artificial Intelligence:0.3978124260902405\n","Classifying text with BART topic classification method\n","Highest-scored label = Computer Science:0.2049967646598816\n","Classifying text with BART topic classification method\n","Highest-scored label = Artificial Intelligence:0.3340168595314026\n","Classifying text with BART topic classification method\n","Highest-scored label = Computer Science:0.24279990792274475\n","Classifying text with BART topic classification method\n","Highest-scored label = Artificial Intelligence:0.2519596517086029\n","Classifying text with BART topic classification method\n","Highest-scored label = Statistics:0.22498369216918945\n","Classifying text with BART topic classification method\n","Highest-scored label = Artificial Intelligence:0.4121207892894745\n","Classifying text with BART topic classification method\n","Highest-scored label = Computer Science:0.19858109951019287\n","Classifying text with BART topic classification method\n","Highest-scored label = Computer Science:0.28164607286453247\n","Classifying text with BART topic classification method\n","Highest-scored label = Artificial Intelligence:0.4653548002243042\n","Classifying text with BART topic classification method\n","Highest-scored label = Neuroscience:0.7402055859565735\n","Classifying text with BART topic classification method\n","Highest-scored label = Computer Science:0.2403641790151596\n","Classifying text with BART topic classification method\n","Highest-scored label = Physics:0.32108864188194275\n","Classifying text with BART topic classification method\n","Highest-scored label = Mathematics:0.21398572623729706\n","Classifying text with BART topic classification method\n","Highest-scored label = Statistics:0.8176688551902771\n","Classifying text with BART topic classification method\n","Highest-scored label = Mathematics:0.32701462507247925\n","Classifying text with BART topic classification method\n","Highest-scored label = Physics:0.29626840353012085\n","Classifying text with BART topic classification method\n","Highest-scored label = Physics:0.4082041084766388\n","Classifying text with BART topic classification method\n","Highest-scored label = Mathematics:0.30130574107170105\n","Classifying text with BART topic classification method\n","Highest-scored label = Statistics:0.17446072399616241\n","Classifying text with BART topic classification method\n","Highest-scored label = Physics:0.2440038025379181\n","Classifying text with BART topic classification method\n","Highest-scored label = Mathematics:0.18635739386081696\n","Classifying text with BART topic classification method\n","Highest-scored label = Mathematics:0.6968693733215332\n","Classifying text with BART topic classification method\n","Highest-scored label = Statistics:0.5774661302566528\n","Classifying text with BART topic classification method\n","Highest-scored label = Mathematics:0.35465505719184875\n","Classifying text with BART topic classification method\n","Highest-scored label = Physics:0.26185017824172974\n","Classifying text with BART topic classification method\n","Highest-scored label = Mathematics:0.33219656348228455\n","Classifying text with BART topic classification method\n","Highest-scored label = Physics:0.24945703148841858\n","Classifying text with BART topic classification method\n","Highest-scored label = Computer Science:0.2583829462528229\n","Classifying text with BART topic classification method\n","Highest-scored label = Physics:0.16701139509677887\n","Classifying text with BART topic classification method\n","Highest-scored label = Physics:0.22133129835128784\n","Classifying text with BART topic classification method\n","Highest-scored label = Statistics:0.31165605783462524\n","Classifying text with BART topic classification method\n","Highest-scored label = Physics:0.2504384517669678\n","Classifying text with BART topic classification method\n","Highest-scored label = Mathematics:0.22044381499290466\n","Classifying text with BART topic classification method\n","Highest-scored label = Neuroscience:0.7049564719200134\n","Classifying text with BART topic classification method\n","Highest-scored label = Mathematics:0.4740028381347656\n","Classifying text with BART topic classification method\n","Highest-scored label = Mathematics:0.26644712686538696\n","Model accuracy: 68.0%\n"]}]},{"cell_type":"markdown","source":["The BART-Large_MNLI topic classification model achieves an accuracy of x when evaluated with a uniform random sample of 50 papers."],"metadata":{"id":"do7Y215rVXNr"}},{"cell_type":"markdown","source":["# Classification with T0pp (T-Zero Plus Plus)\n","\n","From the Big Science project - \"T0 shows zero-shot task generalization on English natural language prompts, outperforming GPT-3 on many tasks, while being 16x smaller.\" \n","\n","In this section, I try out T0pp for the AI-related paper classification task. \n","\n","(This was added about 3 days after I submitted the final version of the project - consider it an extra I added just for fun/testing). \n","\n","NOTE: Thsi "],"metadata":{"id":"GFx-adjWqmLd"}},{"cell_type":"code","source":["# imports for this section\n","from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"],"metadata":{"id":"aFvh85R1rTk2","executionInfo":{"status":"ok","timestamp":1654128236537,"user_tz":240,"elapsed":140,"user":{"displayName":"jeremy h","userId":"13438703414107206507"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["\n","# tokenizer = AutoTokenizer.from_pretrained(\"bigscience/T0pp\")\n","# model = AutoModelForSeq2SeqLM.from_pretrained(\"bigscience/T0pp\")\n","\n","# prompt = \"Is this paper about artificial intelligence or not?\"\n","\n","# review = train_data.data[0]\n","\n","# print(review)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":860,"referenced_widgets":["2883ce25738b44e2899d05586ca9bf9f","c802bb07a7304a7b812d7ad3fbe2a437","86b15c274b104c1ab5e2c49d3b666be4","e88e2b653148494390d7f9d6c8b981fb","09d68d1608394d569c17d5e019c2ac5f","6cee53b817934dd59bda8a32b3402dc4","fea06615f074445e842758af727a8ecf","80203640dd8443e8b971e57d87459454","1afeb392d0624522b20b93e3d631c44d","14115e42184e4bd7ae3658ea74cc958d","c91554ee41a04207b11ad72ba3d2527e"]},"id":"mpiBJ6YPs66K","executionInfo":{"status":"error","timestamp":1654129287262,"user_tz":240,"elapsed":1049618,"user":{"displayName":"jeremy h","userId":"13438703414107206507"}},"outputId":"7f686a69-de54-4a3d-af4d-c33c895567a1"},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/41.5G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2883ce25738b44e2899d05586ca9bf9f"}},"metadata":{}},{"output_type":"error","ename":"OSError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mget_from_cache\u001b[0;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 585\u001b[0;31m             \u001b[0mhttp_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_to_download\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresume_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers)\u001b[0m\n\u001b[1;32m    439\u001b[0m             \u001b[0mprogress\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m             \u001b[0mtemp_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m     \u001b[0mprogress\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/tempfile.py\u001b[0m in \u001b[0;36mfunc_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    619\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mfunc_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m             \u001b[0;31m# Avoid closing the file as long as the wrapper is alive,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOSError\u001b[0m: [Errno 28] No space left on device","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   1859\u001b[0m                     \u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1860\u001b[0;31m                     \u001b[0muser_agent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muser_agent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1861\u001b[0m                 )\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_path\u001b[0;34m(url_or_filename, cache_dir, force_download, proxies, resume_download, user_agent, extract_compressed_file, force_extract, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m    289\u001b[0m             \u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 290\u001b[0;31m             \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_files_only\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    291\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mget_from_cache\u001b[0;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 585\u001b[0;31m             \u001b[0mhttp_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_to_download\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresume_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/tempfile.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exc, value, tb)\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOSError\u001b[0m: [Errno 28] No space left on device","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-7de1ce22c284>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bigscience/T0pp\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForSeq2SeqLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bigscience/T0pp\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Is this paper about artificial intelligence or not?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    444\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m             \u001b[0mmodel_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_model_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m         raise ValueError(\n\u001b[1;32m    448\u001b[0m             \u001b[0;34mf\"Unrecognized configuration class {config.__class__} for this kind of AutoModel: {cls.__name__}.\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   1941\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1942\u001b[0m                 raise EnvironmentError(\n\u001b[0;32m-> 1943\u001b[0;31m                     \u001b[0;34mf\"Can't load the model for '{pretrained_model_name_or_path}'. If you were trying to load it from \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1944\u001b[0m                     \u001b[0;34m\"'https://huggingface.co/models', make sure you don't have a local directory with the same name. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1945\u001b[0m                     \u001b[0;34mf\"Otherwise, make sure '{pretrained_model_name_or_path}' is the correct path to a directory \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOSError\u001b[0m: Can't load the model for 'bigscience/T0pp'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'bigscience/T0pp' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack."]}]},{"cell_type":"code","source":["\n","inputs = tokenizer.encode(\"Is this review positive or negative? Review: this is the best cast iron skillet you will ever buy\", return_tensors=\"pt\")\n","outputs = model.generate(inputs)\n","print(tokenizer.decode(outputs[0]))"],"metadata":{"id":"45dOj1a9tKmE"},"execution_count":null,"outputs":[]}]}