{"text": "out of plane effect on the superconductivity of sr2 xbaxcuo3+d with tc up to 98k. we comment on the paper published by w.b. gao q.q. liu l.x. yang y.yu f.y. li c.q. jin and s. uchida in phys. rev. b and give alternate explanations for the enhanced superconductivity. the enhanced onset tc of 98k observed upon substituting ba for sr is attributed to optimal oxygen ordering rather than to the increase in volume. comparison with la2cuo +x samples suggest that the effect of disorder is overestimated.", "meta": {"id": "1002.1725", "year": 2010}, "label": "Not AI"}
{"text": "level density of + states in 40ca from high energy resolution p p experiments. the level density of + states in 40ca has been extracted in the energy region of the isoscalar giant quadrupole resonance isgqr from a fluctuation analysis of high energy resolution p p data taken at incident energies of mev at the k600 magnetic spectrometer of ithemba labs south africa. quasi free scattering cross sections were calculated to estimate their role as a background contribution to the spectra and found to be small. the shape of the background was determined from the discrete wavelet transform of the spectra using a biorthogonal wavelet function normalized at the lowest particle separation threshold. the experimental results are compared to widely used phenomenological and microscopic models.", "meta": {"id": "1110.1586", "year": 2011}, "label": "Not AI"}
{"text": "dbscan for nonlinear equalization in high capacity multi carrier optical communications. coherent optical multi carrier communications have recently dominated metro regional and long haul optical communications. however the major obstacle of networks involving coherent multi carrier signals such as coherent optical orthogonal frequency division multiplexing co ofdm is the fiber induced nonlinearity and the parametric noise amplification from cascaded optical amplifiers which results in significant nonlinear distortion among subcarriers. here we present the first nonlinear equalizer in optical communications using the traditional density based spatial clustering of applications with noise dbscan algorithm and a novel modified version of dbscan which combines k means clustering on the noisy un clustered symbols. for a gbit sec differential quaternary phase shift keying dqpsk co ofdm system the modified dbscan can increase the signal quality factor by up to db compared to linear equalization at km of transmission. the modified dbscan slightly outperforms the traditional dbscan fuzzy logic c means hierarchical and conventional k means clustering at high launched optical powers.", "meta": {"id": "1902.01198", "year": 2019}, "label": "Not AI"}
{"text": "spherical collapse in the extended quintessence cosmological models. we use the spherical collapse method to investigate the non linear density perturbations of pressureless matter in the cosmological models with the extended quintessence as dark energy in the metric and palatini formalisms. we find that for both formalisms when the coupling constant is negative the deviation from the cdm model is the least according to the evolutionary curves of the linear density contrast and virial overdensity and it is less than one percent. and this indicates that in the extended quintessence cosmological models in which the coupling constant is negative all quantities dependent on or are essentially unaffected if the linear density contrast or the virial overdensity of the cdm model is used as an approximation. moreover we find that the differences between different formalisms are very small in terms of structure formation and thus can not be used to distinguish the metric and palatini formalisms.", "meta": {"id": "1510.04010", "year": 2015}, "label": "Not AI"}
{"text": "rdec integrating regularization into deep embedded clustering for imbalanced datasets. clustering is a fundamental machine learning task and can be used in many applications. with the development of deep neural networks dnns combining techniques from dnns with clustering has become a new research direction and achieved some success. however few studies have focused on the imbalanced data problem which commonly occurs in real world applications. in this paper we propose a clustering method regularized deep embedding clustering rdec that integrates virtual adversarial training vat a network regularization technique with a clustering method called deep embedding clustering dec . dec optimizes cluster assignments by pushing data more densely around centroids in latent space but it is sometimes sensitive to the initial location of centroids especially in the case of imbalanced data where the minor class has less chance to be assigned a good centroid. rdec introduces regularization using vat to ensure the model s robustness to local perturbations of data. vat pushes data that are similar in the original space closer together in the latent space bunching together data from minor classes and thereby facilitating cluster identification by rdec. combining the advantages of dec and vat rdec attains state of the art performance on both balanced and imbalanced benchmark real world datasets. for example accuracies are as high as on mnist dataset and on a highly imbalanced dataset derived from the mnist which is nearly higher than the current best result.", "meta": {"id": "1812.02293", "year": 2018}, "label": "Not AI"}
{"text": "effects due to backscattering and pseudogap features in graphene nanoribbons with single vacancies. we present a systematic study of electron backscattering phenomena during conduction for graphene nanoribbons with single vacancy scatterers and dimensions within the capabilities of modern lithographic techniques. our analysis builds upon an textit ab initio parameterized semiempirical model that breaks electron hole symmetry and nonequilibrium green s function methods for the calculation of the conductance distribution . the underlying mechanism is based on wavefunction localizations and perturbations that in the case of the first plateau can give rise to impurity like pseudogaps with both donor and acceptor characteristics. confinement and geometry are crucial for the manifestation of such effects. self consistent quantum transport calculations characterize vacancies as local charging centers that can induce electrostatic inhomogeneities on the ribbon topology.", "meta": {"id": "1002.0949", "year": 2010}, "label": "Not AI"}
{"text": "atomic scale relaxation dynamics and aging in a metallic glass probed by x ray photon correlation spectroscopy. we use x ray photon correlation spectroscopy to investigate the structural relaxation process in a metallic glass on the atomic length scale. we report evidence for a dynamical crossover between the supercooled liquid phase and the metastable glassy state suggesting different origins of the relaxation process across the transition. furthermore using different cooling rates we observe a complex hierarchy of dynamic processes characterized by distinct aging regimes. strong analogies with the aging dynamics of soft glassy materials such as gels and concentrated colloidal suspensions point at stress relaxation as a universal mechanism driving the relaxation dynamics of out of equilibrium systems.", "meta": {"id": "1209.2030", "year": 2012}, "label": "Not AI"}
{"text": "two level protein folding optimization on a three dimensional ab off lattice model. this paper presents a two level protein folding optimization on a three dimensional ab off lattice model. the first level is responsible for forming conformations with a good hydrophobic core or a set of compact hydrophobic amino acid positions. these conformations are forwarded to the second level where an accurate search is performed with the aim of locating conformations with the best energy value. the optimization process switches between these two levels until the stopping condition is satisfied. an auxiliary fitness function was designed for the first level while the original fitness function is used in the second level. the auxiliary fitness function includes expression about the quality of the hydrophobic core. this expression is crucial for leading the search process to the promising solutions that have a good hydrophobic core and consequently improves the efficiency of the whole optimization process. our differential evolution algorithm was used for demonstrating the efficiency of the two level optimization. it was analyzed on well known amino acid sequences that are used frequently in the literature. the obtained experimental results show that the employed two level optimization improves the efficiency of our algorithm significantly and that the proposed algorithm is superior to other state of the art algorithms.", "meta": {"id": "1903.01456", "year": 2019}, "label": "Not AI"}
{"text": "a hebbian anti hebbian network for online sparse dictionary learning derived from symmetric matrix factorization. olshausen and field of proposed that neural computations in the primary visual cortex v1 can be partially modeled by sparse dictionary learning. by minimizing the regularized representation error they derived an online algorithm which learns gabor filter receptive fields from a natural image ensemble in agreement with physiological experiments. whereas the of algorithm can be mapped onto the dynamics and synaptic plasticity in a single layer neural network the derived learning rule is nonlocal the synaptic weight update depends on the activity of neurons other than just pre and postsynaptic ones and hence biologically implausible. here to overcome this problem we derive sparse dictionary learning from a novel cost function a regularized error of the symmetric factorization of the input s similarity matrix. our algorithm maps onto a neural network of the same architecture as of but using only biologically plausible local learning rules. when trained on natural images our network learns gabor filter receptive fields and reproduces the correlation among synaptic weights hard wired in the of network. therefore online symmetric matrix factorization may serve as an algorithmic theory of neural computation.", "meta": {"id": "1503.00690", "year": 2015}, "label": "Not AI"}
{"text": "limiting distributions of spectral radii for product of matrices from the spherical ensemble. consider the product of independent random matrices from the spherical ensemble for . the spectral radius is defined as the maximum absolute value of the eigenvalues of the product matrix. when the limiting distribution for the spectral radii has been obtained by jiang and qi . in this paper we investigate the limiting distributions for the spectral radii in general. when is a fixed integer we show that the spectral radii converge weakly to distributions of functions of independent gamma random variables. when tends to infinity as goes to infinity we show that the logarithmic spectral radii have a normal limit.", "meta": {"id": "1801.06877", "year": 2018}, "label": "Not AI"}
{"text": "antiferromagnetism in iron based superconductors selection of magnetic order and quasiparticle interference. the recent discovery of superconductivity in the iron based layered pnictides with t c ranging between and 56k generated enormous interest in the physics of these materials. here we review some of the peculiarities of the antiferromagnetic order in the iron pnictides including the selection of the stripe magnetic order and the formation of the ising nematic state in the unfolded bz within an itinerant description. in addition we analyze the properties of the quasiparticle interference spectrum in the parent antiferromagnetic phase.", "meta": {"id": "1402.7206", "year": 2014}, "label": "Not AI"}
{"text": "learning adversarially fair and transferable representations. in this paper we advocate for representation learning as the key to mitigating unfair prediction outcomes downstream. motivated by a scenario where learned representations are used by third parties with unknown objectives we propose and explore adversarial representation learning as a natural method of ensuring those parties act fairly. we connect group fairness demographic parity equalized odds and equal opportunity to different adversarial objectives. through worst case theoretical guarantees and experimental validation we show that the choice of this objective is crucial to fair prediction. furthermore we present the first in depth experimental demonstration of fair transfer learning and demonstrate empirically that our learned representations admit fair predictions on new tasks while maintaining utility an essential goal of fair representation learning.", "meta": {"id": "1802.06309", "year": 2018}, "label": "Not AI"}
{"text": "chord generation from symbolic melody using blstm networks. generating a chord progression from a monophonic melody is a challenging problem because a chord progression requires a series of layered notes played simultaneously. this paper presents a novel method of generating chord sequences from a symbolic melody using bidirectional long short term memory blstm networks trained on a lead sheet database. to this end a group of feature vectors composed of semitones is extracted from the notes in each bar of monophonic melodies. in order to ensure that the data shares uniform key and duration characteristics the key and the time signatures of the vectors are normalized. the blstm networks then learn from the data to incorporate the temporal dependencies to produce a chord progression. both quantitative and qualitative evaluations are conducted by comparing the proposed method with the conventional hmm and dnn hmm based approaches. proposed model achieves and performance increase from the other models respectively. user studies further confirm that the chord sequences generated by the proposed method are preferred by listeners.", "meta": {"id": "1712.01011", "year": 2017}, "label": "Not AI"}
{"text": "joint effects of context and user history for predicting online conversation re entries. as the online world continues its exponential growth interpersonal communication has come to play an increasingly central role in opinion formation and change. in order to help users better engage with each other online we study a challenging problem of re entry prediction foreseeing whether a user will come back to a conversation they once participated in. we hypothesize that both the context of the ongoing conversations and the users previous chatting history will affect their continued interests in future engagement. specifically we propose a neural framework with three main layers each modeling context user history and interactions between them to explore how the conversation context and user chatting history jointly result in their re entry behavior. we experiment with two large scale datasets collected from twitter and reddit. results show that our proposed framework with bi attention achieves an f1 score of on twitter conversations outperforming the state of the art methods from previous work.", "meta": {"id": "1906.01185", "year": 2019}, "label": "Not AI"}
{"text": "euclid definition study report. euclid is a space based survey mission from the european space agency designed to understand the origin of the universe s accelerating expansion. it will use cosmological probes to investigate the nature of dark energy dark matter and gravity by tracking their observational signatures on the geometry of the universe and on the cosmic history of structure formation. the mission is optimised for two independent primary cosmological probes weak gravitational lensing wl and baryonic acoustic oscillations bao . the euclid payload consists of a m korsch telescope designed to provide a large field of view. it carries two instruments with a common field of view of ~ deg2 the visual imager vis and the near infrared instrument nisp which contains a slitless spectrometer and a three bands photometer. the euclid wide survey will cover deg2 of the extragalactic sky and is complemented by two deg2 deep fields. for wl euclid measures the shapes of resolved galaxies per arcmin2 in one broad visible r+i+z band nm . the photometric redshifts for these galaxies reach a precision of dz +z < . they are derived from three additional euclid nir bands y j h in the range micron complemented by ground based photometry in visible bands derived from public data or through engaged collaborations. the bao are determined from a spectroscopic survey with a redshift accuracy dz +z =. the slitless spectrometer with spectral resolution ~ predominantly detects ha emission line galaxies. euclid is a medium class mission of the esa cosmic vision programme with a foreseen launch date in . this report also known as the euclid red book describes the outcome of the phase a study.", "meta": {"id": "1110.3193", "year": 2011}, "label": "Not AI"}
{"text": "compactness result and its applications in integral equations. a version of arzel `a ascoli theorem for being locally compact hausdorff space is proved. the result is used in proving compactness of fredholm hammerstein and urysohn operators. two fixed point theorems for hammerstein and urysohn operator are derived on the basis of schauder theorem.", "meta": {"id": "1505.02533", "year": 2015}, "label": "Not AI"}
{"text": "from nosql accumulo to newsql graphulo design and utility of graph algorithms inside a bigtable database. google bigtable s scale out design for distributed key value storage inspired a generation of nosql databases. recently the newsql paradigm emerged in response to analytic workloads that demand distributed computation local to data storage. many such analytics take the form of graph algorithms a trend that motivated the graphblas initiative to standardize a set of matrix math kernels for building graph algorithms. in this article we show how it is possible to implement the graphblas kernels in a bigtable database by presenting the design of graphulo a library for executing graph algorithms inside the apache accumulo database. we detail the graphulo implementation of two graph algorithms and conduct experiments comparing their performance to two main memory matrix math systems. our results shed insight into the conditions that determine when executing a graph algorithm is faster inside a database versus an external system in short that memory requirements and relative i o are critical factors.", "meta": {"id": "1606.07085", "year": 2016}, "label": "Not AI"}
{"text": "higher order and mixed qcd qed corrections for drell yan. in this article we discuss about the drell yan process focusing on the computation of radiative corrections for vector boson production. first we describe the resummation formalism and its application to obtain higher order qcd corrections. we briefly summarize the state of the art in these calculations and motivate the need of more refined theoretical predictions. after that we center into the inclusion of mixed qcd qed higher order contributions both for fixed order and resummed calculations. in particular we present a framework based on the resummation formalism to simultaneously handle soft gluon and photon radiation. finally we discuss future extensions of this approach and its phenomenological relevance.", "meta": {"id": "1908.02209", "year": 2019}, "label": "Not AI"}
{"text": "on the quiver presentation of the descent algebra of the symmetric group. we describe a presentation for the descent algebra of the symmetric group as a quiver with relations. this presentation arises from a new construction of the descent algebra as a homomorphic image of an algebra of forests of binary trees which can be identified with a subspace of the free lie algebra. in this setting we provide a new short proof of the known fact that the quiver of the descent algebra of is given by restricted partition refinement. moreover we describe certain families of relations and conjecture that for fixed the finite set of relations from these families that are relevant for the descent algebra of generates the ideal of relations and hence yields an explicit presentation by generators and relations of the algebra.", "meta": {"id": "1206.0327", "year": 2012}, "label": "Not AI"}
{"text": "first passage under restart with branching. first passage under restart with branching is proposed as a generalization of first passage under restart. strong motivation to study this generalization comes from the observation that restart with branching can expedite the completion of processes that cannot be expedited with simple restart yet a sharp and quantitative formulation of this statement is still lacking. we develop a comprehensive theory of first passage under restart with branching. this reveals that two widely applied measures of statistical dispersion the coefficient of variation and the gini index come together to determine how restart with branching affects the mean completion time of an arbitrary stochastic process. the universality of this result is demonstrated and its connection to extreme value theory is also pointed out and explored.", "meta": {"id": "1807.09363", "year": 2018}, "label": "Not AI"}
